{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import random as rd  # generating random numbers for distributions\n",
    "import pandas as pd  # data mangling and transforming\n",
    "import numpy as np  # handling vectors and matrices\n",
    "import matplotlib.pyplot as plt  # plotting module\n",
    "import seaborn as sns  # pairplot & heatmap for correlations\n",
    "import graphviz  # dendograms\n",
    "import sympy  # linear dependencies\n",
    "import torch  # neural net\n",
    "\n",
    "from math import sqrt  # mathematical operators\n",
    "from sklearn import tree, linear_model  # lm + regression & classification trees\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, PolynomialFeatures # preprocessing tools\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesClassifier, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inside notebook\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'MTTF': np.random.choice(range(0, 3500), n),\n",
    "                        'longitude': np.random.choice(range(10, 50), n),\n",
    "                        'latitude': np.random.choice(range(10, 50), n),\n",
    "                        'A': np.append(np.random.choice(range(1, 5), n-1), np.nan),\n",
    "                        'B': np.random.choice(range(1, 500000), n),\n",
    "                        'C': np.random.choice(range(0, 50000), n),\n",
    "                        'D': np.random.choice(['Sam', 'Som', 'Sim'], n),\n",
    "                        'E': np.random.choice([''.join(rd.choices(string.ascii_uppercase, k=5)) \n",
    "                                               for i in range(10)], n),\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe after removing variables with variance below threshold\n",
    "def varianceSelection(X, THRESHOLD = .1):\n",
    "    sel = VarianceThreshold(threshold=THRESHOLD)\n",
    "    sel.fit_transform(X)\n",
    "    return X[[c for (s, c) in zip(sel.get_support(), X.columns.values) if s]]\n",
    "\n",
    "\n",
    "# calculate own score robustly\n",
    "def calc_zscore(col, ddof=0):\n",
    "    return (col - col.mean())/col.std(ddof=ddof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset(s)\n",
    "\n",
    "### inverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('inverters.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index to be udid\n",
    "df.index = df.UDID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional columns\n",
    "for v in ['MEAN', 'MEDIAN', 'MIN', 'MAX']:\n",
    "    df.loc[:,'ACDC_RATIO_'+v] = df.loc[:,'AC_POWER_'+v] / df.loc[:,'DC_POWER_'+v]\n",
    "    df.loc[:,'AC_POWER_NORM_'+v] = df.loc[:,'AC_POWER_'+v] / df.loc[:,'POWER_NOMINAL']\n",
    "    df.loc[:,'DC_POWER_NORM_'+v] = df.loc[:,'DC_POWER_'+v] / df.loc[:,'POWER_NOMINAL']\n",
    "    df.loc[:,'TEMP_DIFF_'+v] = df.loc[:,'TEMPERATURE_'+v] / df.loc[:,'FELTTEMPERATURE_'+v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of variable with most missings by creating a dummy\n",
    "df['DC_MISSING_DUMMY'] = df.DC_VOLTAGE_MEAN.isnull()*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorization of columns\n",
    "names_drop = ['UDID', 'DEVICE_TYPE']\n",
    "names_y1 = ['MEAN_TTF']\n",
    "names_y2 = ['AVAILABILITY', 'MEDIAN_TTF', 'STD_TTF', 'MEAN_TTR',\n",
    "            'MEDIAN_TTR', 'STD_TTR', 'MEAN_TBF', 'MEDIAN_TBF', 'STD_TBF']\n",
    "names_X = list(set(df.columns) - set(names_drop) - set(names_y1) - set(names_y2))\n",
    "names_disc = ['SOLAR_CLIMATE_ZONE', 'VENDOR', 'POWER_NOMINAL_RANGE', 'POWER_MAX_RANGE']\n",
    "names_median = []\n",
    "for n in names_X:\n",
    "    if 'MEDIAN' in n:\n",
    "        names_median.append(n)\n",
    "names_acdc = []\n",
    "for n in names_X:\n",
    "    if 'AC' in n or 'DC' in n:\n",
    "        names_acdc.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(names_drop, inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Notizen\n",
    "Variablen seit 2017:\n",
    "- DIF\n",
    "- DNI\n",
    "- ETR\n",
    "- FELTT\n",
    "- GHI\n",
    "- GNI\n",
    "- HUMIDITY\n",
    "- SLP\n",
    "- TEMP\n",
    "\n",
    "Wichtige Air Quality Variablen:\n",
    "- AQI\n",
    "- pm10\n",
    "- pm25\n",
    "- o3\n",
    "- no2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "aqi = pd.read_csv('aqi_out.csv', sep=',')\n",
    "aqi.index = aqi.UDID\n",
    "aqi = aqi[['AQI','pm10','pm25','o3','no2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join together by index\n",
    "df = df.join(aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only consider inverters which had at least 1 failure!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.AVAILABILITY!=100]\n",
    "\n",
    "\n",
    "## Exploratory View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "print('MEAN')\n",
    "plt.boxplot(df.MEAN_TTF)\n",
    "plt.show()\n",
    "print('MEDIAN')\n",
    "plt.boxplot(df.MEDIAN_TTF)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### Pairplot\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# # Create pairplot for first groups of median variables\n",
    "# for i in list(range(8,12,4)):\n",
    "#     pp = sns.pairplot(df[names_y1+names_median[i-3:i]+['POWER_NOMINAL_RANGE']], hue='POWER_NOMINAL_RANGE')\n",
    "#     pp.savefig('plots/pp_'+str(i)+'.png') \n",
    "\n",
    "\n",
    "# ### Correlation Matrix\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "corr = df.corr()\n",
    "sns.set(rc={'figure.figsize':(19,16)})\n",
    "correlation_matrix = sns.heatmap(corr,\n",
    "                                 cmap='RdBu',\n",
    "                                 xticklabels=corr.columns.values,\n",
    "                                 yticklabels=corr.columns.values,\n",
    "                                 annot=False)\n",
    "fig = correlation_matrix.get_figure()\n",
    "fig.savefig('plots/correlation_matrix_raw.png') \n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# remove other ys\n",
    "df.drop(names_y2, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# move y variable to first place\n",
    "cols = list(df)\n",
    "cols.insert(0, cols.pop(cols.index(names_y1[0])))\n",
    "# use loc to reorder\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "corr = df.corr()\n",
    "sns.set(rc={'figure.figsize':(19,16)})\n",
    "correlation_matrix = sns.heatmap(corr,\n",
    "                                 cmap='RdBu',\n",
    "                                 xticklabels=corr.columns.values,\n",
    "                                 yticklabels=corr.columns.values,\n",
    "                                 annot=False)\n",
    "fig = correlation_matrix.get_figure()\n",
    "fig.savefig('plots/correlation_matrix_raw_sorted.png') \n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# save top 10 correlated X\n",
    "top_by_lincorr = list(abs(corr[names_y1[0]]).sort_values()[-15:-5].index)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "top_by_lincorr\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# Create pairplot for relevant variables\n",
    "pp = sns.pairplot(df[names_y1+top_by_lincorr[5:]+['POWER_NOMINAL_RANGE']], hue='POWER_NOMINAL_RANGE')\n",
    "pp.savefig('plots/pp_top.png') \n",
    "\n",
    "\n",
    "# ## Data Cleaning\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# new df object\n",
    "df_clean = df\n",
    "\n",
    "\n",
    "# ### Filter known mistakes\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# delete AC mistakes\n",
    "df_clean = df_clean[df_clean.AC_POWER_MEDIAN.notnull()]\n",
    "\n",
    "\n",
    "# ### Missing Values\n",
    "\n",
    "# #### Imputing (by domain knowledge)\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "# use raw approximation, if other two values are available for DC\n",
    "for v in ['MEAN', 'MEDIAN', 'MIN', 'MAX']:\n",
    "    df_clean['DC_VOLTAGE_'+v].fillna(df_clean['DC_POWER_'+v]/df_clean['DC_CURRENT_'+v], inplace=True)\n",
    "    df_clean['DC_CURRENT_'+v].fillna(df_clean['DC_POWER_'+v]/df_clean['DC_VOLTAGE_'+v], inplace=True)\n",
    "    df_clean['DC_POWER_'+v].fillna(df_clean['DC_CURRENT_'+v]*df_clean['DC_VOLTAGE_'+v], inplace=True)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# user power nominal for few missing values\n",
    "df_clean['PPEAK'].fillna(df_clean['POWER_NOMINAL'], inplace=True)\n",
    "df_clean['POWER_MAX'].fillna(df_clean['POWER_NOMINAL'], inplace=True)\n",
    "df_clean['POWER_MAX_RANGE'].fillna(df_clean['POWER_NOMINAL_RANGE'], inplace=True)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# transform infs to nan\n",
    "df_clean = df_clean.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# #### Imputing (by median)\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "check_nulls = df_clean.isnull().sum()\n",
    "names_mi = list(check_nulls[check_nulls > 0].index)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "for c in names_mi:\n",
    "    df_clean[c].fillna(df_clean[c].median(), inplace=True)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# double checking\n",
    "df_clean.isnull().values.any()\n",
    "\n",
    "\n",
    "# ### Outlier\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "dfo = df_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "dfoz = dfo.apply(calc_zscore, axis=0)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "# variables with zero variance\n",
    "dfoz.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "dfo = dfo[(np.abs(dfoz) < 3).all(axis=1)]\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "# join back non-numerical features\n",
    "df_clean = dfo.join(df_clean.select_dtypes(exclude=[np.number]))\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "print(str(len(df) - len(df_clean))+' row(s) got kicked out!')\n",
    "\n",
    "\n",
    "# ### Dummy vars for non-numeric features\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "df_clean = pd.get_dummies(df_clean, drop_first=True)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "df_clean.head()\n",
    "\n",
    "\n",
    "# ## Feature Cleaning\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "X0 = df_clean.drop(names_y1, axis=1)\n",
    "\n",
    "\n",
    "# ### 1 - Drop features with low variance\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "X1 = varianceSelection(X0, THRESHOLD=0.05)  # Variance at least 0.05\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "to_drop1 = list(set(X0.columns) - set(X1.columns))\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "print(str(len(to_drop1))+' features got kicked out!')\n",
    "\n",
    "\n",
    "# ### 2 - Drop highly correlated features\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = X1.corr().abs()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop2 = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "# Drop features \n",
    "X2 = X1.drop(to_drop2, axis=1)\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "print(str(len(to_drop2))+' features got kicked out!')\n",
    "\n",
    "\n",
    "# ### 3 - Drop features which are linearly dependent\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "# reduced_form, inds = sympy.Matrix(X2.values).rref()\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "# X3 = X2.iloc[:, list(inds)]\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "print('We end up with %d features'%X2.shape[1])\n",
    "\n",
    "\n",
    "# ## Feature Normalization & Scaling\n",
    "\n",
    "# ### Normalize\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "normalizer = Normalizer().fit(X2)\n",
    "X_norm = pd.DataFrame(normalizer.transform(X2), columns=X2.columns)\n",
    "\n",
    "\n",
    "# ### Scale\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "scaler_mm = MinMaxScaler()\n",
    "X_scalem = pd.DataFrame(scaler_mm.fit_transform(X2), columns=X2.columns)\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "scaler_s = StandardScaler()\n",
    "X_scales = pd.DataFrame(scaler_s.fit_transform(X2), columns=X2.columns)\n",
    "\n",
    "\n",
    "# ## Feature Selection\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# define dependent and independent variables\n",
    "y = df_clean[names_y1[0]]\n",
    "y_log = np.log(y)\n",
    "X = X_scalem\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "# build 100 trees & show variable importance\n",
    "clf = RandomForestRegressor(n_estimators=100)\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "top_by_rf = []\n",
    "for f in range(X.shape[1]):\n",
    "    top_by_rf.append(X.columns[indices[f]])\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "\n",
    "# **The importances add up to 100% and are calculated based on variance reduction achievable by each feature.**\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "fig = plt.figure()\n",
    "plt.title(\"Feature importances by Random Forest\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"b\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), top_by_rf, rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "fig.savefig('plots/rf_feature_importances.png') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## Polynomial Features\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "X_reduced_by_rf = X_scalem[top_by_rf[:4]]\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X_reduced_by_rf)\n",
    "\n",
    "\n",
    "# ## Models\n",
    "\n",
    "# ### Simple linear regression model: w = beta_hat*X + e\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "X_reduced = X[top_by_rf]\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "# Create linear regression object with normalized variables as benchmark\n",
    "linreg = linear_model.LinearRegression(normalize=True)\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "linreg.fit(X_reduced, y)\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_lr = linreg.predict(X_reduced)\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', linreg.coef_)\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error (RMSE): %.2f\"\n",
    "      % sqrt(mean_squared_error(y, y_lr)))\n",
    "# R^2\n",
    "print('R² score: %.2f' % linreg.score(X_reduced, y))\n",
    "print('Adj. R² score: %.2f' % (1-(1-linreg.score(X_reduced, y))*(len(y)-1)/(len(y)-X_reduced.shape[1]-1)))\n",
    "\n",
    "\n",
    "# ### Regression Tree\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "# create object\n",
    "dtr = tree.DecisionTreeRegressor(criterion='mse', max_depth=8)\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "# fit data\n",
    "dtr.fit(X, y)\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "# Predict (in-sample)\n",
    "y_dt = dtr.predict(X)\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "dot_data = tree.export_graphviz(dtr, out_file=None,\n",
    "                                feature_names=list(X.columns),\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"plots/Regression Tree\", format='png') \n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "print('RMSE for 1 decision tree: %4f'%sqrt(mean_squared_error(y, y_dt)))\n",
    "\n",
    "\n",
    "# ### Gradient boosted forest\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "# create object\n",
    "gbr = GradientBoostingRegressor(n_estimators=500, max_depth=8, \n",
    "                                min_samples_split=2, learning_rate=0.01,\n",
    "                                loss='huber')\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "# fit data\n",
    "gbr.fit(X, y)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "# predict (in-sample)\n",
    "y_gb = gbr.predict(X)\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "\n",
    "print('RMSE for Gradient-Boosted Forest (500): %.4f'%sqrt(mean_squared_error(y, y_gb)))\n",
    "\n",
    "\n",
    "# ### Ada-boosted Trees\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "\n",
    "# create object\n",
    "rng = np.random.RandomState(1)\n",
    "abr = AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=8),\n",
    "                        learning_rate=0.01, loss='linear',\n",
    "                        n_estimators=500, random_state=rng)\n",
    "\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "\n",
    "# fit data\n",
    "abr.fit(X, y)\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "# Predict (in-sample)\n",
    "y_ab = abr.predict(X)\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "print('RMSE for 500 ada boosted-trees: %4f'%sqrt(mean_squared_error(y, y_ab)))\n",
    "\n",
    "\n",
    "# ### Neural Net\n",
    "\n",
    "# #### Data\n",
    "\n",
    "# In[285]:\n",
    "\n",
    "\n",
    "# data\n",
    "inputs = Variable(torch.from_numpy(np.array(X, dtype='float32')))\n",
    "target = Variable(torch.from_numpy(np.array(y, dtype='float32')))\n",
    "\n",
    "\n",
    "# In[286]:\n",
    "\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(TensorDataset(inputs, target), \n",
    "                      batch_size, \n",
    "                      shuffle=False,\n",
    "                      num_workers=6)\n",
    "\n",
    "\n",
    "# #### Define layers of the net\n",
    "\n",
    "# In[287]:\n",
    "\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "    nn.Linear(inputs.shape[1], 50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(50, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# #### define optimizer & loss function\n",
    "\n",
    "# In[288]:\n",
    "\n",
    "\n",
    "opt = torch.optim.SGD(nn_model.parameters(), lr=1e-07)\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "\n",
    "# In[289]:\n",
    "\n",
    "\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "loss_fn = RMSELoss\n",
    "\n",
    "\n",
    "# #### train the model\n",
    "\n",
    "# In[290]:\n",
    "\n",
    "\n",
    "# Define a utility function to train the model\n",
    "def fit(df, num_epochs, model, loss_fn, opt):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in df:\n",
    "            # Generate predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            # Perform gradient descent\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if epoch%10==0:\n",
    "            print(\"Epoch %s: Loss %s\"%(epoch, loss.data.item()))\n",
    "    print('Training loss: ', loss_fn(model(inputs), target).data.item())\n",
    "\n",
    "\n",
    "# In[291]:\n",
    "\n",
    "\n",
    "# Train the model for X epochs\n",
    "fit(train_dl, 50, nn_model, loss_fn, opt)\n",
    "\n",
    "\n",
    "# In[292]:\n",
    "\n",
    "\n",
    "preds = nn_model(inputs)\n",
    "\n",
    "\n",
    "# In[293]:\n",
    "\n",
    "\n",
    "# save predictions as array\n",
    "y_nn = preds.detach().numpy().squeeze()\n",
    "\n",
    "\n",
    "# In[294]:\n",
    "\n",
    "\n",
    "print('RMSE for Neural Net: %4f'%sqrt(mean_squared_error(y, y_nn)))\n",
    "\n",
    "\n",
    "# In[295]:\n",
    "\n",
    "\n",
    "plt.hist(y_nn)\n",
    "\n",
    "\n",
    "# ### Ensemble Predictions\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y_ens = np.mean([y_dt, y_gb, y_ab, y_nn], axis=0)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('RMSE for Ensemble Predictions: %4f'%sqrt(mean_squared_error(y, y_ens)))\n",
    "\n",
    "\n",
    "# ## Evaluation\n",
    "\n",
    "# ### Descriptive plots\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# create data_frame of predictions and true values\n",
    "df_preds = pd.DataFrame(data={'y_true':y, 'y_lr':y_lr, 'y_dt':y_dt, \n",
    "                              'y_gb':y_gb, 'y_ab':y_ab, 'y_nn':y_nn})\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# add power range\n",
    "df_preds = df_preds.join(df.POWER_NOMINAL_RANGE)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# pairplots\n",
    "ppp = sns.pairplot(df_preds, hue='POWER_NOMINAL_RANGE')\n",
    "ppp.savefig('plots/pp_predictions.png') \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# coordinates for origin line\n",
    "ol = list(range(0, 40000, 5000))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "olp = sns.lmplot(x='y_nn', y='y_true', data=df_preds, fit_reg=False, \n",
    "           hue='POWER_NOMINAL_RANGE', legend=False, palette=\"Set1\")\n",
    "plt.plot(ol, ol, linewidth=2)\n",
    "plt.legend(loc='lower right')\n",
    "olp.savefig('plots/true_vs_predictions.png') \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "resp = sns.residplot(x='y_gb', y='y_true', data=df_preds)\n",
    "resp.figure.savefig('plots/residuals.png') \n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
